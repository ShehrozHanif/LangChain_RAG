{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Install Required Libraries**"
      ],
      "metadata": {
        "id": "3CYOt_o6DMeV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpOG5IasCstg",
        "outputId": "ef78aacf-a782-47c2-9f8d-a9e1cf536389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs or updates the langchain-pinecone and langchain-google-genai libraries, which are essential for integrating LangChain with Pinecone and Google's Gemini AI, respectively."
      ],
      "metadata": {
        "id": "xppFaE0lCxDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Import Libraries and Configure Pinecone**"
      ],
      "metadata": {
        "id": "EOVRByh-DbSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "GJZw3R3xDaeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**.**from google.colab import userdata:**\n",
        "\n",
        "◘ Google Colab provides a built-in module called userdata that securely stores\n",
        "and retrieves sensitive information, like API keys.\n",
        "\n",
        "◘ This helps you avoid hardcoding sensitive information (like keys) in your code, keeping it safe.\n",
        "\n",
        "\n",
        "**2**. **from pinecone import Pinecone, ServerlessSpec:**\n",
        "\n",
        "◘ This imports the Pinecone library's Pinecone and ServerlessSpec classes.\n",
        "\n",
        "◘ Pinecone is a service used for building vector databases, where data is stored and retrieved as numerical vectors for machine learning purposes.\n",
        "\n",
        "◘ ServerlessSpec helps configure Pinecone's serverless database setup (cloud provider, region, etc.).\n",
        "\n",
        "\n",
        "**3**. **pinecone_api_key = userdata.get('PINECONE_API_KEY'):**\n",
        "\n",
        "◘ This retrieves your Pinecone API key from the secure userdata locker.\n",
        "\n",
        "◘ The key is stored under the label 'PINECONE_API_KEY'.\n",
        "\n",
        "◘ You need this key to authenticate with Pinecone and prove you have permission to use its services.\n",
        "\n",
        "**4**. **pc = Pinecone(api_key=pinecone_api_key):**\n",
        "\n",
        "◘ This initializes a Pinecone client (pc) using the API key retrieved in the previous step.\n",
        "\n",
        "◘ The client (pc) is what you use to interact with the Pinecone database, such as creating indexes, storing vectors, and searching data.\n",
        "\n"
      ],
      "metadata": {
        "id": "u4gp_CClDhXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Create Pinecone Index**"
      ],
      "metadata": {
        "id": "1kPuoCXaH8DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"online-rag-project2\"\n",
        "\n",
        "pc.create_index(\n",
        "     name=index_name,\n",
        "     dimension=768,\n",
        "     metric=\"cosine\",\n",
        "     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "\n",
        " )\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "U2gMY0JOH7TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step creates a Pinecone index named \"online-rag-project\" with a vector dimension of 768 and a cosine similarity metric. The ServerlessSpec specifies the cloud provider and region. After creation, you initialize the index for further operations.\n",
        "\n",
        "An index in Pinecone is like a specialized storage unit for vectors. It's where all your data (transformed into numerical vectors) will be stored and retrieved efficiently.\n",
        "\n",
        "**Purpose of an Index:**\n",
        "\n",
        "To store vectors in a structured way.\n",
        "To enable fast similarity searches or queries, like finding the closest matches to a given vector.\n",
        "\n",
        "\n",
        "**What Does This Code Achieve?**\n",
        "\n",
        "**Creates an Index**: Named \"online-rag-project\", capable of storing vectors of size 768.\n",
        "\n",
        "**Sets Up for Vector Similarity:** Uses cosine similarity to find related vectors.\n",
        "\n",
        "**Hosts on the Cloud:** Deploys on AWS in the us-east-1 region for efficient storage and retrieval."
      ],
      "metadata": {
        "id": "jGh1S114H6tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Set Up Google Gemini AI Embeddings**"
      ],
      "metadata": {
        "id": "21mgggjfNam-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "ngbacIpPNbO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Does This Code Do?**\n",
        "\n",
        "**1.Import the Required Class:**\n",
        "GoogleGenerativeAIEmbeddings is imported to generate embeddings.\n",
        "\n",
        "**Set the API Key:**The API key is retrieved from the userdata and set as an environment variable.\n",
        "This is necessary for authenticating your requests to Google’s AI models.\n",
        "\n",
        "**Initialize the Embedding Model:**\n",
        "The embeddings object is created using the GoogleGenerativeAIEmbeddings class, with the specified model (models/embedding-001).\n",
        "\n",
        "**Key Difference: Direct Use vs. Environment Variables**\n",
        "\n",
        "**Direct Use:**\n",
        "\n",
        "◘ In Step 2, userdata.get directly fetches and uses the API key, which is passed explicitly to the Pinecone client.\n",
        "\n",
        "◘ No environment variable is needed because Pinecone’s API directly supports passing the key.\n",
        "\n",
        "**Environment Variables:**\n",
        "\n",
        "◘ In this step, the embedding library expects the key to exist as an environment variable. This is a requirement of the langchain-google-genai library.\n",
        "\n",
        "◘ By setting the key in os.environ, you satisfy this requirement.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "◘ os: Manages environment variables to securely handle sensitive data like API keys.\n",
        "\n",
        "◘ Embeddings: Convert raw text into numerical vectors that capture its meaning.\n",
        "\n",
        "◘ Purpose: This step sets up the embedding model to generate vectors, which form the backbone of your RAG (Retrieval-Augmented Generation) system."
      ],
      "metadata": {
        "id": "SsdML1deNbsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Generate Embedding for a Query**"
      ],
      "metadata": {
        "id": "P1axQNAOUmTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Hello world\")\n"
      ],
      "metadata": {
        "id": "iO07EWxXUjgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line generates a vector embedding for the query \"Hello world\" using the previously initialized embeddings model.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "◘ Vector Embedding converts text into a numerical format (a vector) that preserves the meaning of the text.\n",
        "\n",
        "◘ Vectors help machines understand and compare the meaning of different pieces of text, like words or sentences.\n",
        "\n",
        "◘ In your code, embed_query(\"Hello world\") is creating a vector representation of the sentence \"Hello world,\" which will be useful for tasks like searching, similarity comparison, and more."
      ],
      "metadata": {
        "id": "yqHSFh2SW0nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Inspect the Generated Vector**"
      ],
      "metadata": {
        "id": "9MI6owckXHCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OkcxfBNW1Ax",
        "outputId": "24243a6b-9181-4277-d7b8-73e503df94b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04703257977962494,\n",
              " -0.04019005596637726,\n",
              " -0.02902696467936039,\n",
              " -0.026809632778167725,\n",
              " 0.01892058178782463]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you display the first five elements of the generated vector to inspect its structure.\n",
        "\n",
        "\n",
        "**vector:** This refers to the vector generated by the embed_query(\"Hello world\") function, which is a list (or array) of numbers representing the sentence \"Hello world\" in a multi-dimensional space.\n",
        "\n",
        "**[:5]**: This is Python slice notation, which extracts the first five elements of the vector. It means:\n",
        "\n",
        "Start from the beginning of the vector (index 0).\n",
        "Stop at index 5 (but not include it, so you get the elements at indices 0, 1, 2, 3, and 4).\n",
        "\n",
        "This allows you to inspect the first few numbers in the vector, which is helpful for understanding the structure of the embedding and getting a sense of how the model represents the input data numerically."
      ],
      "metadata": {
        "id": "nbpZSzVeXHzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Initialize Pinecone Vector Store**"
      ],
      "metadata": {
        "id": "3c6QWQngiHea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "xNJyebG2XGMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You import the PineconeVectorStore class and initialize it with the Pinecone index and embeddings model. This setup allows for efficient storage and retrieval of vector embeddings.\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "**Pinecone** is a vector database that efficiently stores and retrieves high-dimensional vectors (like those generated from text using an embedding model).\n",
        "\n",
        "**LangChain** abstracts and simplifies working with such databases. In this step, we initialize a PineconeVectorStore, which connects the Pinecone database with our embeddings model (Google’s model here).\n",
        "\n",
        "**The purpose of this step** is to set up a system where you can store and search for vector representations of text data using Pinecone and LangChain."
      ],
      "metadata": {
        "id": "YeIj55pxiIFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Create Document Instances**"
      ],
      "metadata": {
        "id": "waySAkuylWAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")"
      ],
      "metadata": {
        "id": "8XIyfl8UiIyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You import the Document class and create an instance document_1 with sample text and metadata indicating the source.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "**Instance**: document_1 is an instance of the Document class. It represents a specific document with its content and metadata.\n",
        "\n",
        "**Class:** The Document class is like a blueprint or template that defines how a document should look.\n",
        "\n",
        "**Purpose:** You're using this step to create a document with text (page_content) and metadata (source), so it can be stored, processed, and searched later."
      ],
      "metadata": {
        "id": "5NGXVKaslWYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9: Display Document**"
      ],
      "metadata": {
        "id": "2H1hVrQjmATf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4_cOcHBmA7k",
        "outputId": "b2228ea1-6928-4a79-d5bb-e5a1aaa7033e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'tweet'}, page_content='I had chocalate chip pancakes and scrambled eggs for breakfast this morning.')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line displays the document_1 instance, showing its content and metadata.\n",
        "\n"
      ],
      "metadata": {
        "id": "5ocwxKJbmM8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10: Create Multiple Document Instances**"
      ],
      "metadata": {
        "id": "xogNGRjbmQHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Se4hRvimQgb",
        "outputId": "030b7b4b-0c1b-4eb5-979f-66468657b68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You create multiple Document instances with varied content and metadata, then compile them into a list documents. Finally, you check the length of the list to confirm the number of documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "qLqZbZIomRKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 11: Generate Unique Identifiers**"
      ],
      "metadata": {
        "id": "NyPodVXFovuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "uuid4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk3Mu4mxo0MZ",
        "outputId": "a7fa38f7-28ed-4175-c0dc-f138eaa3ffc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('5de7b2cc-090e-4a42-9e9d-8390c587dc14')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You import the uuid4 function and generate a unique identifier. This is useful for assigning unique IDs to documents.\n",
        "\n",
        "◘ UUID stands for Universally Unique Identifier. It's a 128-bit number used to uniquely identify information in computer systems.\n",
        "\n",
        "◘ uuid4() is a method from Python's uuid module that generates a random UUID.\n",
        "\n",
        "◘ When working with collections of documents, data, or objects, it's often important to ensure that each one can be uniquely identified. This is where UUIDs come into play.\n"
      ],
      "metadata": {
        "id": "klLZXGvPo0mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 12: Assign Unique IDs and Add Documents to Vector Store**"
      ],
      "metadata": {
        "id": "54KmfLoXqV_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tp5soAGqXKy",
        "outputId": "a6e87916-6e1d-405b-d50f-fb42a3434bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7e05acec-dc99-4d9f-b203-993e4456ceef',\n",
              " 'a41bc81d-4d54-4b58-8c7b-5f79830fd95f',\n",
              " 'a13d2df4-7e22-484e-8aab-1dfb6aed5be0',\n",
              " '159a328a-49af-4db0-b8af-ccd0e2f7b47e',\n",
              " '2a8856d5-6ad1-4e1d-ae43-bbe8ad0c785b',\n",
              " 'e7e6e36a-b5ed-4c53-985a-884c365cc7c0',\n",
              " '994fbc81-f165-4bcb-a56e-b6e7f1c4dfee',\n",
              " '3c7fc3d0-1503-4448-be0a-bb9731116ab4',\n",
              " '5d011055-f3e6-40d1-8c2e-5e028d220c32',\n",
              " 'a27d7a4a-62f6-4fd4-a67f-defeb6df2a96']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You generate a list of unique IDs for each document and add the documents to the vector store with their corresponding IDs.\n",
        "\n",
        "Storage: Databases or APIs often store IDs as strings because it is a universal format that can be easily indexed and retrieved."
      ],
      "metadata": {
        "id": "H29BklsrqW5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 13: Perform Similarity Search with Filter**"
      ],
      "metadata": {
        "id": "lgqPv5fdug_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Retrieve\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txENIxItqWcf",
        "outputId": "3b10a660-7067-48c8-8244-d969873d6964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You perform a similarity search in the vector store for documents related to the query \"LangChain provides abstractions to make working with LLMs easy,\" retrieving the top 2 results filtered by the source \"tweet.\" Then, you print the content and metadata of each result.\n",
        "\n",
        "**How It Works:**\n",
        "\n",
        "1. **Query to Vector Conversion:** The input query \"LangChain provides abstractions to make working with LLMs easy\" is first converted into a vector using the embedding model (e.g., GoogleGenerativeAIEmbeddings).\n",
        "\n",
        "2. **Similarity Calculation:** Pinecone compares the query vector to the stored vectors and finds the two most similar vectors to the query (as specified by k=2). It uses a distance metric (like cosine similarity) to measure the closeness between vectors.\n",
        "\n",
        "3. **Filter: The filter** ({\"source\": \"tweet\"}) ensures that only documents tagged with the \"source\": \"tweet\" metadata are included in the search results.\n",
        "\n",
        "4. **Displaying Results:** After the similarity search is done, the code loops over the results and prints the page_content (text content) and metadata (additional info like source) of the top two most similar documents."
      ],
      "metadata": {
        "id": "x2JQWD3nv4Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 14: Perform Similarity Search with Scores and Filter**"
      ],
      "metadata": {
        "id": "W0W6SIWqzgxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6NYUtC7uhtG",
        "outputId": "233b0b80-c069-4c17-8f2c-d982b79daed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.667716] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How It Works:**\n",
        "\n",
        "**Step 1:** **Embedding the Query:**  The query \"Will it be hot tomorrow?\" is passed through the embeddings model (e.g., GoogleGenerativeAIEmbeddings). This model converts the query into a vector representation.\n",
        "\n",
        "**Step 2:** **Vector Comparison:** The query vector is then compared to all the vectors stored in the Pinecone vector store (or any other vector store). The comparison uses a distance metric (often cosine similarity) to measure how similar each stored vector is to the query vector.\n",
        "\n",
        "**Step 3: Returning Results:T**he method returns the document that has the highest similarity to the query. Along with the document, it also returns the similarity score, which indicates how closely the document matches the query. The score can be a value between 0 and 1 (depending on the similarity metric used)."
      ],
      "metadata": {
        "id": "2NLC-42GznJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 15: Initialize Google Gemini AI Model**"
      ],
      "metadata": {
        "id": "gsknfFcjuiGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "oxRLLR3n1UCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you import the ChatGoogleGenerativeAI class and initialize it with the \"gemini-1.5-flash\" model. You set parameters such as temperature, max_tokens, timeout, and max_retries to control the model's behavior.\n",
        "\n",
        "\n",
        "1. **Parameters Used in ChatGoogleGenerativeAI:**\n",
        "\n",
        "**model=\"gemini-1.5-flash\":**\n",
        "This specifies the particular AI model you're using, which in this case is \"gemini-1.5-flash\". The model refers to a version of Google's Gemini AI that is specialized for handling certain tasks, such as text generation and conversational AI.\n",
        "\n",
        "**temperature=0:**\n",
        "This controls the creativity of the responses. A lower temperature (like 0) results in more deterministic and focused responses, while higher values (e.g., 1) allow for more random or creative answers.\n",
        "In this case, setting temperature=0 ensures that the model gives more consistent and logical responses.\n",
        "\n",
        "**max_tokens=None:**\n",
        "This specifies the maximum number of tokens (i.e., words or characters) the model should generate in a response. Setting it to None means there’s no explicit limit, so the model can generate as many tokens as needed based on the context.\n",
        "\n",
        "**timeout=None:**\n",
        "This defines the time limit for a request to complete. If set to None, it means there is no specific timeout limit.\n",
        "\n",
        "**max_retries=2:**\n",
        "This indicates that the system will retry the request up to 2 times in case of failures (e.g., network issues or timeouts).\n",
        "The model will attempt the same operation again before giving up, ensuring more robust performance.\n",
        "\n",
        "2. **Use Case:**\n",
        "\n",
        "**Natural Language Processing (NLP):** The ChatGoogleGenerativeAI class is useful when you need to integrate Google's powerful generative AI into your application. It allows you to perform tasks such as:\n",
        "\n",
        "**Text generation:** Creating content based on a prompt (e.g., writing articles, blogs, summaries).\n",
        "\n",
        "**Question answering:** Using the model to answer user queries based on a dataset or context.\n",
        "\n",
        "**Conversational AI:** Engaging in chat-like interactions where the AI responds to user inputs in natural language.\n",
        "\n",
        "**Integrating AI into Applications:** By using this class, you can integrate Google's Gemini AI into your applications, making them smarter and more interactive.\n",
        "\n",
        "**For example:**\n",
        "\n",
        "**Customer service bots:** Using the AI to answer customer queries and support tickets automatically.\n",
        "\n",
        "**Content creation tools:** Allowing the AI to assist in generating text-based content for websites, blogs, and other platforms.\n",
        "\n",
        "**Personal assistants:** Creating smart assistants that can converse with users and provide useful information.\n"
      ],
      "metadata": {
        "id": "xjhjFqtC1UqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 16: Define Function to Answer User Questions**"
      ],
      "metadata": {
        "id": "EBSoUNbD1WP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user_question(question: str):\n",
        "\n",
        "  # Vector Search\n",
        "  vector_results = vector_store.similarity_search(question, k=2)\n",
        "  print(len(vector_results))\n",
        "\n",
        "  # Pass to Model Vector Rsults + User Query\n",
        "  final_answer = llm.invoke(f\"ANSWER THIS QUERY: {question},Here are some refrences answer {vector_results}\")\n",
        "\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "3qGSGTDt1VaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You define a function answer_to_user_question that takes a user query as input. It performs a similarity search in the vector store to retrieve the top 2 relevant documents. Then, it passes the user query along with the retrieved documents to the Gemini AI model to generate a final answer."
      ],
      "metadata": {
        "id": "TNvZWs5-5Ndp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 17: Get Answer to a Specific Question**"
      ],
      "metadata": {
        "id": "_EGG6moj5zG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user_question(\"LangChain provides abstractions to make working with LLMs easy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmj3K0iv5zic",
        "outputId": "2df2c145-2e23-4668-c7f4-8502eef73118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You call the answer_to_user_question function with the query \"LangChain provides abstractions to make working with LLMs easy\" and retrieve the generated answer's content."
      ],
      "metadata": {
        "id": "yejDG5Ad5z5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "xBgiEXkj66YJ",
        "outputId": "12df7805-bf32-4f26-ec40-e20d30a9eacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The provided text mentions LangChain, a framework for working with Large Language Models (LLMs), making it easier to use them.  The reference documents are unrelated to this statement; one mentions LangGraph and the other mentions a project using LangChain, but neither directly supports or refutes the claim about LangChain's ease of use.  Therefore, the query about LangChain's ease of use is neither confirmed nor denied by the provided references.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Flow Of  Retrieval-Augmented Generation(RAG)**\n",
        "\n",
        "The code you've provided sets up a Retrieval-Augmented Generation (RAG) system using LangChain, Pinecone, and Google's Gemini AI model. The primary goal is to enhance the capabilities of a language model by integrating it with external data sources, allowing it to provide more accurate and contextually relevant responses.\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1. **Pinecone Vector Store:** Used for storing and retrieving vector embeddings of documents, enabling efficient similarity searches.\n",
        "\n",
        "2. **Google Gemini AI Model:** Generates embeddings and answers user queries based on the retrieved documents.\n",
        "\n",
        "3. **LangChain Framework:** Facilitates the orchestration of the RAG process, managing the flow of data between components.\n",
        "\n",
        "\n",
        "**Process Flow:**\n",
        "\n",
        "1. **Data Preparation:** Documents are created and embedded using the Google Gemini AI model.\n",
        "\n",
        "2. **Indexing:** The embedded documents are stored in the Pinecone vector store.\n",
        "\n",
        "3. **Query Processing:** When a user query is received, a similarity search is performed in the vector store to retrieve relevant documents.\n",
        "\n",
        "4. **Answer Generation:** The retrieved documents, along with the user query, are passed to the Gemini AI model to generate a final answer.\n",
        "\n",
        "By combining these components, the system can provide responses that are both accurate and contextually relevant, leveraging external data sources to augment the language model's knowledge."
      ],
      "metadata": {
        "id": "mwXxVjtv6LC0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcCeIGcK6KAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yMBp13Yt1Xea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When a user ask a query then how it's work see in the Example\n",
        "\n",
        "**Example Flow:**\n",
        "\n",
        "1. **Text Input:** You provide the query \"What is the weather like today?\".\n",
        "\n",
        "2. **Vectorization:** This query is converted into a vector, like [0.3456, -0.7892, 0.2345, ...].\n",
        "\n",
        "3. **Search:** Pinecone searches for the most similar vectors to the query vector in the database.\n",
        "\n",
        "4. **Matching Vectors:** Pinecone finds the vectors that are closest to the query vector and returns their UUIDs.\n",
        "\n",
        "5. **Result:** You retrieve the documents associated with those UUIDs (e.g., \"The weather forecast is cloudy with a chance of rain\"), which is your result."
      ],
      "metadata": {
        "id": "p1THHOrOue5z"
      }
    }
  ]
}